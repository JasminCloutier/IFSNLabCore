{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import json\n",
    "import ast\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "## NLP\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus.reader import CorpusReader\n",
    "from nltk.internals import deprecated\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.util import binary_search_file as _binary_search_file\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "w_tokenizer = WhitespaceTokenizer()\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "from nostril import nonsense\n",
    "from polyglot.detect import Detector\n",
    "from polyglot.detect.base import logger as polyglot_logger\n",
    "polyglot_logger.setLevel(\"ERROR\")\n",
    "from textblob import TextBlob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanup functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations = []\n",
    "for punct in string.punctuation:\n",
    "    if punct not in [\"#\", \"@\"]:\n",
    "        punctuations.append(punct)\n",
    "\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "punctuations_stopwords = []\n",
    "for punct in punctuations:\n",
    "    if punct not in punctuations_stopwords:\n",
    "        punctuations_stopwords.append(punct)  \n",
    "for stopword in stopWords:\n",
    "    if stopword not in punctuations_stopwords:\n",
    "        punctuations_stopwords.append(stopword)\n",
    "\n",
    "def deEmojify(text):\n",
    "    regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    return regrex_pattern.sub(r'',text)\n",
    "\n",
    "def get_clean_text(x):\n",
    "    if type(x) is str:\n",
    "        x = x.lower()\n",
    "        #remove user handles\n",
    "        x = re.sub('@[^\\s]+','',x)\n",
    "        #regex to remove emails\n",
    "        x = re.sub(r'([a-zA-Z0-9+._-]+@[a-zA-Z0-9._-]+\\.[a-zA-Z0-9_-]+)', '', x) \n",
    "        #regex to remove URLs\n",
    "        x = re.sub(r'(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?', '', x)\n",
    "        #remove \"RT\"\n",
    "        x = re.sub('rt', '', x)\n",
    "        #remove unk tokens\n",
    "        x = re.sub('unk', '', x)\n",
    "        #removing anything that's not alphabets\n",
    "        x = re.sub('[^A-Z a-z]+', '', x)\n",
    "        #remove elipses from end of words\n",
    "        x = re.sub(\"\\.\\.\\.\", '', x)\n",
    "        x = re.sub(\"\\.\\.\", '', x)\n",
    "        return x\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "def remove_nonenglish(text):\n",
    "    try:\n",
    "        if Detector(text).language.code == \"en\":\n",
    "            return text\n",
    "        else:\n",
    "            return \"\"\n",
    "    except: # errors are likely due to short length, so append anyway\n",
    "        return text\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w.lower()) for w in w_tokenizer.tokenize(text)]\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return [i for i in text if i not in punctuations_stopwords]\n",
    "\n",
    "def remove_spaces(text):\n",
    "    response = []\n",
    "    for i in text:\n",
    "        if i !=\" \":\n",
    "            response.append(i.strip())\n",
    "    return response\n",
    "\n",
    "def remove_nonsense(text):\n",
    "    response = []\n",
    "    if text is not None:\n",
    "        if len(text) > 0:\n",
    "            for i in text:\n",
    "                try:\n",
    "                    if not nonsense(i):\n",
    "                        response.append(i)\n",
    "                except:\n",
    "                    response.append(i)\n",
    "    return response\n",
    "\n",
    "def pipeline(text):\n",
    "    no_emojis = text.apply(deEmojify)\n",
    "    clean = no_emojis.apply(lambda x: get_clean_text(x))\n",
    "    text_english = clean.apply(remove_nonenglish)\n",
    "    text_lemmatized = text_english.apply(lemmatize_text) # Now text is a list\n",
    "    text_unpunctuated = text_lemmatized.apply(remove_punctuation) \n",
    "    text_nospace = text_unpunctuated.apply(remove_spaces)\n",
    "    text_sense = text_nospace.apply(remove_nonsense)\n",
    "    return text_sense\n",
    "\n",
    "def pipeline_bert(text):\n",
    "    no_emojis = text.apply(deEmojify)\n",
    "    clean = no_emojis.apply(lambda x: get_clean_text(x))\n",
    "    text_nospace = clean.apply(lambda x: re.sub(r\"\\s+\", \" \", x))\n",
    "    text_string = text_nospace.apply(lambda x: re.sub(r\"\\.\", \"[CLS] [SEP]\", x))\n",
    "    text_string = text_string.apply(lambda x: \"[CLS] \" + x + \" [SEP]\")\n",
    "    return text_string\n",
    "\n",
    "def pipeline_LIWC(text):\n",
    "    no_emojis = text.apply(deEmojify)\n",
    "    clean = no_emojis.apply(lambda x: get_clean_text(x))\n",
    "    text_nospace = clean.apply(lambda x: re.sub(r\"\\s+\", \" \", x))\n",
    "    return text_nospace\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word dictionary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synonyms(word_list):\n",
    "    new = []\n",
    "    for text in word_list:\n",
    "        new.append(text)\n",
    "        for syn in wordnet.synsets(text):\n",
    "            if syn.pos() in [\"a\", \"s\", \"r\"]:\n",
    "                word = syn.name().split(\".\")[0]\n",
    "                if word not in new:\n",
    "                    new.append(word)\n",
    "    for word in new:\n",
    "        lemma = lemmatizer.lemmatize(word)\n",
    "        if lemma not in new:\n",
    "            new.append(lemma)\n",
    "    final = []\n",
    "    for word in new:\n",
    "        if word not in final:\n",
    "            final.append(word)\n",
    "    return final\n",
    "\n",
    "def get_adjectives(word_list):\n",
    "    new = []\n",
    "    for text in word_list:\n",
    "        new.append(text)\n",
    "        for syn in wordnet.synsets(text):\n",
    "            also_sees = syn.also_sees()\n",
    "            if len(also_sees) > 0:\n",
    "                for seealso in also_sees:\n",
    "                    if seealso.pos() in [\"a\", \"s\", \"r\"]:\n",
    "                        word = seealso.name().split(\".\")[0]\n",
    "                        new.append(word)\n",
    "            similar_tos = syn.similar_tos()\n",
    "            if len(similar_tos) > 0:\n",
    "                for similar in similar_tos:\n",
    "                    if similar.pos() in [\"a\", \"s\", \"r\"]:\n",
    "                        word = similar.name().split(\".\")[0]\n",
    "                        new.append(word)\n",
    "            attributes = syn.attributes()\n",
    "            if len(attributes) > 0:\n",
    "                for attribute in attributes:\n",
    "                    if attribute.pos() in [\"a\", \"s\", \"r\"]:\n",
    "                        word = attribute.name().split(\".\")[0]\n",
    "    for word in new:\n",
    "        lemma = lemmatizer.lemmatize(word)\n",
    "        if lemma not in new:\n",
    "            new.append(lemma)\n",
    "    final = []\n",
    "    for word in new:\n",
    "        if word not in final:\n",
    "            final.append(word)\n",
    "    return final\n",
    "    \n",
    "\n",
    "def get_hypernyms(word_list):\n",
    "    new = []\n",
    "    for text in word_list:\n",
    "        new.append(text)\n",
    "        for syn in wordnet.synsets(text):\n",
    "            hypernyms = syn.hypernyms()\n",
    "            if len(hypernyms) > 0:\n",
    "                for hypernym in hypernyms:\n",
    "                    if hypernym.pos() in [\"a\", \"s\", \"r\"]:\n",
    "                        word = hypernym.name().split(\".\")[0]\n",
    "                        new.append(word)\n",
    "    for word in new:\n",
    "        lemma = lemmatizer.lemmatize(word)\n",
    "        if lemma not in new:\n",
    "            new.append(lemma)\n",
    "    final = []\n",
    "    for word in new:\n",
    "        if word not in final:\n",
    "            final.append(word)\n",
    "    return final\n",
    "\n",
    "def get_hyponyms(word_list):\n",
    "    new = []\n",
    "    for text in word_list:\n",
    "        new.append(text)\n",
    "        for syn in wordnet.synsets(text):\n",
    "            hyponyms = syn.hyponyms()\n",
    "            if len(hyponyms) > 0:\n",
    "                for hyponym in hyponyms:\n",
    "                    if hyponym.pos() in [\"a\", \"s\", \"r\"]:\n",
    "                        word = hyponym.name().split(\".\")[0]\n",
    "                        new.append(word)\n",
    "    for word in new:\n",
    "        lemma = lemmatizer.lemmatize(word)\n",
    "        if lemma not in new:\n",
    "            new.append(lemma)\n",
    "    final = []\n",
    "    for word in new:\n",
    "        if word not in final:\n",
    "            final.append(word)\n",
    "    return final\n",
    "\n",
    "def get_holonyms(word_list):\n",
    "    new = []\n",
    "    for text in word_list:\n",
    "        new.append(text)\n",
    "        for syn in wordnet.synsets(text):\n",
    "            member_holonyms = syn.member_holonyms()\n",
    "            if len(member_holonyms) > 0:\n",
    "                for holonym in member_holonyms:\n",
    "                    if holonym.pos() in [\"a\", \"s\", \"r\"]:\n",
    "                        word = holonym.name().split(\".\")[0]\n",
    "                        new.append(word)\n",
    "        for syn in wordnet.synsets(text):\n",
    "            substance_holonyms = syn.substance_holonyms()\n",
    "            if len(substance_holonyms) > 0:\n",
    "                for holonym in substance_holonyms:\n",
    "                    if holonym.pos() in [\"a\", \"s\", \"r\"]:\n",
    "                        word = holonym.name().split(\".\")[0]\n",
    "                        new.append(word)\n",
    "        for syn in wordnet.synsets(text):\n",
    "            part_holonyms = syn.part_holonyms()\n",
    "            if len(part_holonyms) > 0:\n",
    "                for holonym in part_holonyms:\n",
    "                    if holonym.pos() in [\"a\", \"s\", \"r\"]:\n",
    "                        word = holonym.name().split(\".\")[0]\n",
    "                        new.append(word)\n",
    "    for word in new:\n",
    "        lemma = lemmatizer.lemmatize(word)\n",
    "        if lemma not in new:\n",
    "            new.append(lemma)\n",
    "    final = []\n",
    "    for word in new:\n",
    "        if word not in final:\n",
    "            final.append(word)\n",
    "    return final\n",
    "\n",
    "def get_meronyms(word_list):\n",
    "    new = []\n",
    "    for text in word_list:\n",
    "        new.append(text)\n",
    "        for syn in wordnet.synsets(text):\n",
    "            member_meronyms = syn.member_meronyms()\n",
    "            if len(member_meronyms) > 0:\n",
    "                for meronym in member_meronyms:\n",
    "                    if meronym.pos() in [\"a\", \"s\", \"r\"]:\n",
    "                        word = meronym.name().split(\".\")[0]\n",
    "                        new.append(word)\n",
    "        for syn in wordnet.synsets(text):\n",
    "            substance_meronyms = syn.substance_meronyms()\n",
    "            if len(substance_meronyms) > 0:\n",
    "                for meronym in substance_meronyms:\n",
    "                    if meronym.pos() in [\"a\", \"s\", \"r\"]:\n",
    "                        word = meronym.name().split(\".\")[0]\n",
    "                        new.append(word)\n",
    "        for syn in wordnet.synsets(text):\n",
    "            part_meronyms = syn.part_meronyms()\n",
    "            if len(part_meronyms) > 0:\n",
    "                for meronym in part_meronyms:\n",
    "                    if meronym.pos() in [\"a\", \"s\", \"r\"]:\n",
    "                        word = meronym.name().split(\".\")[0]\n",
    "                        new.append(word)\n",
    "    for word in new:\n",
    "        lemma = lemmatizer.lemmatize(word)\n",
    "        if lemma not in new:\n",
    "            new.append(lemma)\n",
    "    final = []\n",
    "    for word in new:\n",
    "        if word not in final:\n",
    "            final.append(word)\n",
    "    return final\n",
    "    \n",
    "def get_derivatives(word_list):\n",
    "    new = []\n",
    "    for text in word_list:\n",
    "        new.append(text)\n",
    "        for syn in wordnet.synsets(text):\n",
    "            if syn.pos() in [\"a\", \"s\", \"r\"]:\n",
    "                lemmas = wordnet.lemmas(syn.name().split(\".\")[0], syn.name().split(\".\")[1])\n",
    "                if len(lemmas) > 0:\n",
    "                    for lemma in lemmas:\n",
    "                        if lemma.syntactic_marker():\n",
    "                            new.append(lemma.name())\n",
    "                        else:\n",
    "                            pass\n",
    "    for word in new:\n",
    "        lemma = lemmatizer.lemmatize(word)\n",
    "        if lemma not in new:\n",
    "            new.append(lemma)\n",
    "    final = []\n",
    "    for word in new:\n",
    "        if word not in final:\n",
    "            final.append(word)\n",
    "    return final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_asian(text_list):\n",
    "    counter = 0\n",
    "    asian_words = [\"asian\", \"asians\", \"chinese\", \"china\", \"wuhan\"]\n",
    "    asian_words_longer = get_synonyms(asian_words)\n",
    "    if type(text_list) == str:\n",
    "        text_list = ast.literal_eval(text_list)\n",
    "    for word in text_list:\n",
    "        if word in asian_words_longer:\n",
    "            counter += 1\n",
    "    return counter\n",
    "\n",
    "def has_chinese(text_list):\n",
    "    counter = 0\n",
    "    chinese_words = [\"chinese\", \"china\", \"wuhan\"]\n",
    "    if type(text_list) == str:\n",
    "        text_list = ast.literal_eval(text_list)\n",
    "    for word in text_list:\n",
    "        if word in chinese_words:\n",
    "            counter += 1\n",
    "    return counter\n",
    "\n",
    "def has_covid(text_list):\n",
    "    counter = 0\n",
    "    covid_words = [\"covid\", \"covid-19\", \"coronavirus\", \"sars-cov-2\"]\n",
    "    if type(text_list) == str:\n",
    "        text_list = ast.literal_eval(text_list)\n",
    "    for word in text_list:\n",
    "        if word in covid_words:\n",
    "            counter += 1\n",
    "    return counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = glob.glob('raw/Full/*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for r in result:\n",
    "#     try:\n",
    "#         df = pd.read_csv(r, lineterminator='\\n')\n",
    "#         df['date'] = r[19:29]\n",
    "#         df['time'] = r[30:38]\n",
    "#         df.to_csv(r, index=False)\n",
    "#     except:\n",
    "#         os.remove(r)\n",
    "#         print(f\"File {r} was deleted because it was empty.\")\n",
    "\n",
    "# for r in result:\n",
    "#     df = pd.read_csv(r, lineterminator='\\n')\n",
    "#     df['lemma'] = pipeline(df.text)\n",
    "#     # Remove empty lines\n",
    "#     df['lemma_length'] = df.lemma.apply(len)\n",
    "#     df = df[df['lemma_length'] > 0]\n",
    "#     count_row = df.shape[0]\n",
    "#     if count_row > 0:\n",
    "#             df.to_csv(r, index=False)\n",
    "\n",
    "# for r in result:\n",
    "#     df = pd.read_csv(r, lineterminator='\\n')\n",
    "#     deduped_df = df[-df[['author_id', 'id']].duplicated()]\n",
    "#     deduped_df.to_csv(r, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_dict = {}\n",
    "\n",
    "# for r in result:\n",
    "#     if r[19:29] in result_dict.keys():\n",
    "#         result_dict[r[19:29]].append(r)\n",
    "#     else:\n",
    "#         result_dict[r[19:29]] = [r]\n",
    "\n",
    "# for key in result_dict.keys():\n",
    "#     output_path = f\"raw/Full/response_{key}.csv\"\n",
    "#     df1 = pd.read_csv(result_dict[key][0], lineterminator='\\n')\n",
    "#     df = df1\n",
    "#     if len(result_dict[key]) > 1:\n",
    "#         df2 = pd.read_csv(result_dict[key][1], lineterminator='\\n')\n",
    "#         df = pd.concat([df1, df2])\n",
    "#     if len(result_dict[key]) > 2:\n",
    "#         df3 = pd.read_csv(result_dict[key][2], lineterminator='\\n')\n",
    "#         df = pd.concat([df1, df2, df3])\n",
    "#     df.to_csv(output_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_users = {}\n",
    "# result = glob.glob('raw/Full/*.csv')\n",
    "# n_users = 0\n",
    "# n_samedayrepeats = 0\n",
    "# maximum = 0\n",
    "# minimum = 1000000000000000000000000000000000000\n",
    "# dates = []\n",
    "\n",
    "# for r in result:\n",
    "#     if r[18:28] not in unique_users.keys():\n",
    "#         df = pd.read_csv(r, lineterminator='\\n')\n",
    "#         users = df.author_id.unique().tolist()\n",
    "#         if len(df.author_id.tolist()) - len(df.author_id.unique().tolist()) != 0:\n",
    "#             dates.append(r[18:28])\n",
    "#             if df.groupby([\"author_id\", \"id\"]).size().max() > maximum:\n",
    "#                 maximum = max(df.groupby([\"author_id\", \"id\"]).size())\n",
    "#             elif df.groupby([\"author_id\", \"id\"]).size().min() < minimum:\n",
    "#                 minimum = min(df.groupby([\"author_id\", \"id\"]).size())\n",
    "            \n",
    "#             n_samedayrepeats += len(df.author_id.tolist()) - len(df.author_id.unique().tolist())\n",
    "            \n",
    "#         n_users += len(users)\n",
    "#         unique_users[r[19:29]] = users\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dates.sort()\n",
    "# dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create columns for BERT and LIWC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = glob.glob('raw/Full/*.csv')\n",
    "\n",
    "# for r in result:\n",
    "#     outfile = f\"{r[:4]}BERT{r[8:]}\"\n",
    "#     df = pd.read_csv(r, lineterminator='\\n')\n",
    "#     df['bert_lemma'] = pipeline_bert(df.text)\n",
    "#     df['LIWC_lemma'] = pipeline_LIWC(df.text)\n",
    "#     df.to_csv(outfile, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create sub dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = glob.glob('raw/BERT/*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for r in result:\n",
    "#     outfile = f\"{r[:17]}Asian/{r[17:]}\"\n",
    "#     if not os.path.exists(outfile):\n",
    "#         df = pd.read_csv(r, lineterminator='\\n')\n",
    "#         df['asian'] = df[\"lemma\"].apply(has_asian)\n",
    "#         df_asian = df[df['asian'] > 0]\n",
    "#         count_row = df_asian.shape[0]\n",
    "#         if count_row > 0:\n",
    "#             df_asian.to_csv(outfile, index=False)\n",
    "#         else:\n",
    "#             print(f\"File {outfile} was not saved because it was empty.\")\n",
    "#     else:\n",
    "#         pass\n",
    "\n",
    "# for r in result:\n",
    "#     outfile = f\"{r[:17]}Chinese/{r[17:]}\"\n",
    "#     if not os.path.exists(outfile):\n",
    "#         df = pd.read_csv(r, lineterminator='\\n')\n",
    "#         df['chinese'] = df[\"lemma\"].apply(has_chinese)\n",
    "#         df_chinese = df[df['chinese'] > 0]\n",
    "#         count_row = df_chinese.shape[0]\n",
    "#         if count_row > 0:\n",
    "#             df_chinese.to_csv(outfile, index=False)\n",
    "#         else:\n",
    "#             print(f\"File {outfile} was not saved because it was empty.\")\n",
    "#     else:\n",
    "#         pass\n",
    "\n",
    "# for r in result:\n",
    "#     outfile = f\"{r[:17]}COVID/{r[17:]}\"\n",
    "#     if not os.path.exists(outfile):\n",
    "#         df = pd.read_csv(r, lineterminator='\\n')\n",
    "#         df['covid'] = df[\"lemma\"].apply(has_covid)\n",
    "#         df_covid = df[df['covid'] > 0]\n",
    "#         count_row = df_covid.shape[0]\n",
    "#         if count_row > 0:\n",
    "#             df_covid.to_csv(outfile, index=False)\n",
    "#         else:\n",
    "#             print(f\"File {outfile} was not saved because it was empty.\")\n",
    "#     else:\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dedupe sub dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_covid = glob.glob('raw/COVID/*.csv')\n",
    "# result_asian = glob.glob('raw/Asian/*.csv')\n",
    "\n",
    "# for r in result_covid:\n",
    "#     df = pd.read_csv(r, lineterminator='\\n')\n",
    "#     deduped_df = df[-df[['author_id', 'id']].duplicated()]\n",
    "#     deduped_df.to_csv(r, index=False)\n",
    "\n",
    "# for r in result_asian:\n",
    "#     df = pd.read_csv(r, lineterminator='\\n')\n",
    "#     deduped_df = df[-df[['author_id', 'id']].duplicated()]\n",
    "#     deduped_df.to_csv(r, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate number of tweets of each type per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = glob.glob('raw/Full/*.csv')\n",
    "# result_covid = glob.glob('raw/COVID/*.csv')\n",
    "# result_asian = glob.glob('raw/Asian/*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for r in result:\n",
    "#     r_new = f\"{r[:8]}/bert{r[8:]}\"\n",
    "#     df = pd.read_csv(r, lineterminator='\\n')\n",
    "#     df['bert_lemma'] = pipeline_bert(df.text)\n",
    "#     df.to_csv(r_new, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_dict = {}\n",
    "\n",
    "# for r in result:\n",
    "#     key = r[18:-4]\n",
    "#     df = pd.read_csv(r, lineterminator='\\n')\n",
    "#     r, c = df.shape\n",
    "#     result_dict[key] = r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame.from_dict(result_dict, orient='index')\n",
    "# df = df.reset_index()\n",
    "# df.columns = [\"day\", \"Number of tweets\"]\n",
    "# df.head()\n",
    "\n",
    "# df.to_csv('TweetsPerDay.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_covid_dict = {}\n",
    "\n",
    "# for r in result_covid:\n",
    "#     key = r[19:29]\n",
    "#     df = pd.read_csv(r, lineterminator='\\n')\n",
    "#     r, c = df.shape\n",
    "#     result_covid_dict[key] = r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame.from_dict(result_covid_dict, orient='index')\n",
    "# df = df.reset_index()\n",
    "# df.columns = [\"day\", \"Number of tweets\"]\n",
    "# df.head()\n",
    "\n",
    "# df.to_csv('COVIDTweetsPerDay.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_asian_dict = {}\n",
    "\n",
    "# for r in result_asian:\n",
    "#     key = r[19:29]\n",
    "#     df = pd.read_csv(r, lineterminator='\\n')\n",
    "#     r, c = df.shape\n",
    "#     result_asian_dict[key] = r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame.from_dict(result_asian_dict, orient='index')\n",
    "# df = df.reset_index()\n",
    "# df.columns = [\"day\", \"Number of tweets\"]\n",
    "# df.head()\n",
    "\n",
    "# df.to_csv('AsianTweetsPerDay.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
