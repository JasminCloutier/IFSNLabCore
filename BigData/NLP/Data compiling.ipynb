{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a621e003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import ast\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from dataclasses import dataclass, field\n",
    "import contextlib\n",
    "\n",
    "from collections import defaultdict, deque\n",
    "from functools import total_ordering\n",
    "from itertools import chain, islice\n",
    "from operator import itemgetter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus.reader import CorpusReader\n",
    "from nltk.internals import deprecated\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.util import binary_search_file as _binary_search_file\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from textblob import TextBlob\n",
    "\n",
    "import sklearn as skl\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(use_idf=True)\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1d8bca",
   "metadata": {},
   "source": [
    "# Custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6c8bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synonyms(word_list):\n",
    "    new = []\n",
    "    for text in word_list:\n",
    "        new.append(text)\n",
    "        for syn in wordnet.synsets(text):\n",
    "            # Hypernyms\n",
    "            hypernyms = syn.hypernyms()\n",
    "            if len(hypernyms) > 0:\n",
    "                for hypernym in hypernyms:\n",
    "                    if hypernym.pos() in [\"a\", \"s\", \"r\"]:\n",
    "                        word = hypernym.name().split(\".\")[0]\n",
    "                        new.append(word)\n",
    "            # Hyponyms\n",
    "            hyponyms = syn.hyponyms()\n",
    "            if len(hyponyms) > 0:\n",
    "                for hyponym in hyponyms:\n",
    "                    if hyponym.pos() in [\"a\", \"s\", \"r\"]:\n",
    "                        word = hyponym.name().split(\".\")[0]\n",
    "                        new.append(word)\n",
    "            # Holonyms\n",
    "            member_holonyms = syn.member_holonyms()\n",
    "            if len(member_holonyms) > 0:\n",
    "                for holonym in member_holonyms:\n",
    "                    if holonym.pos() in [\"a\", \"s\", \"r\"]:\n",
    "                        word = holonym.name().split(\".\")[0]\n",
    "                        new.append(word)\n",
    "            substance_holonyms = syn.substance_holonyms()\n",
    "            if len(substance_holonyms) > 0:\n",
    "                for holonym in substance_holonyms:\n",
    "                    if holonym.pos() in [\"a\", \"s\", \"r\"]:\n",
    "                        word = holonym.name().split(\".\")[0]\n",
    "                        new.append(word)\n",
    "            part_holonyms = syn.part_holonyms()\n",
    "            if len(part_holonyms) > 0:\n",
    "                for holonym in part_holonyms:\n",
    "                    if holonym.pos() in [\"a\", \"s\", \"r\"]:\n",
    "                        word = holonym.name().split(\".\")[0]\n",
    "                        new.append(word)\n",
    "            # Meronyms\n",
    "            member_meronyms = syn.member_meronyms()\n",
    "            if len(member_meronyms) > 0:\n",
    "                for meronym in member_meronyms:\n",
    "                    if meronym.pos() in [\"a\", \"s\", \"r\"]:\n",
    "                        word = meronym.name().split(\".\")[0]\n",
    "                        new.append(word)\n",
    "            substance_meronyms = syn.substance_meronyms()\n",
    "            if len(substance_meronyms) > 0:\n",
    "                for meronym in substance_meronyms:\n",
    "                    if meronym.pos() in [\"a\", \"s\", \"r\"]:\n",
    "                        word = meronym.name().split(\".\")[0]\n",
    "                        new.append(word)\n",
    "            part_meronyms = syn.part_meronyms()\n",
    "            if len(part_meronyms) > 0:\n",
    "                for meronym in part_meronyms:\n",
    "                    if meronym.pos() in [\"a\", \"s\", \"r\"]:\n",
    "                        word = meronym.name().split(\".\")[0]\n",
    "                        new.append(word)\n",
    "            # Also see\n",
    "            also_sees = syn.also_sees()\n",
    "            if len(also_sees) > 0:\n",
    "                for seealso in also_sees:\n",
    "                    if seealso.pos() in [\"a\", \"s\", \"r\"]:\n",
    "                        word = seealso.name().split(\".\")[0]\n",
    "                        new.append(word)\n",
    "            # Similar to\n",
    "            similar_tos = syn.similar_tos()\n",
    "            if len(similar_tos) > 0:\n",
    "                for similar in similar_tos:\n",
    "                    if similar.pos() in [\"a\", \"s\", \"r\"]:\n",
    "                        word = similar.name().split(\".\")[0]\n",
    "                        new.append(word)\n",
    "            # Attributes\n",
    "            attributes = syn.attributes()\n",
    "            if len(attributes) > 0:\n",
    "                for attribute in attributes:\n",
    "                    if attribute.pos() in [\"a\", \"s\", \"r\"]:\n",
    "                        word = attribute.name().split(\".\")[0]\n",
    "                        new.append(word)\n",
    "            # Synonyms\n",
    "            if syn.pos() in [\"a\", \"s\", \"r\"]:\n",
    "                word = syn.name().split(\".\")[0]\n",
    "                if word not in new:\n",
    "                    new.append(word)\n",
    "                # Derivatives\n",
    "                lemmas = wordnet.lemmas(syn.name().split(\".\")[0], syn.name().split(\".\")[1])\n",
    "                if len(lemmas) > 0:\n",
    "                    for lemma in lemmas:\n",
    "                        if lemma.syntactic_marker():\n",
    "                            new.append(lemma.name())\n",
    "                        else:\n",
    "                            pass\n",
    "    for word in new:\n",
    "        lemma = lemmatizer.lemmatize(word)\n",
    "        if lemma not in new:\n",
    "            new.append(lemma)\n",
    "    final = []\n",
    "    for word in new:\n",
    "        if word not in final:\n",
    "            final.append(word)\n",
    "    return final\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f4bb1f",
   "metadata": {},
   "source": [
    "# For GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e69e07f",
   "metadata": {},
   "source": [
    "## Pre-post corpus dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e4f536e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = glob.glob('../raw/Full/*.csv')\n",
    "# result.sort()\n",
    "\n",
    "# result_dict = {}\n",
    "\n",
    "# result_dict[\"pre\"] = {}\n",
    "# result_dict[\"post\"] = {}\n",
    "\n",
    "# response_pre = []\n",
    "# response_deduped_pre = []\n",
    "\n",
    "# for r in result[:324]:\n",
    "#     print(r)\n",
    "#     df = pd.read_csv(r, lineterminator='\\n')\n",
    "#     for row in df['lemma']:\n",
    "#         row = ast.literal_eval(row)\n",
    "#         for word in row:\n",
    "#             if word != \" \":\n",
    "#                 response_pre.append(word)\n",
    "                \n",
    "# # Create deduped list to get length of unique words\n",
    "# for token in tqdm(response_pre):\n",
    "#     if token not in response_deduped_pre:\n",
    "#         response_deduped_pre.append(token)\n",
    "            \n",
    "# result_dict[\"pre\"][\"corpus\"] = response_pre\n",
    "# result_dict[\"pre\"][\"length\"] = len(response_deduped_pre)\n",
    "\n",
    "# with open(\"corpus_prepost.json\", \"w\") as outfile:\n",
    "#     json.dump(result_dict, outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24132af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = open(\"corpus_prepost.json\")\n",
    "# result_dict = json.load(f)\n",
    "# f.close()\n",
    "\n",
    "# response_post = []\n",
    "# response_deduped_post = []\n",
    "\n",
    "# for r in tqdm(result[324:]):\n",
    "#     df = pd.read_csv(r, lineterminator='\\n')\n",
    "#     for row in df['lemma']:\n",
    "#         row = ast.literal_eval(row)\n",
    "#         for word in row:\n",
    "#             if word != \" \":\n",
    "#                 response_post.append(word)\n",
    "\n",
    "# # Create deduped list to get length of unique words\n",
    "# for token in tqdm(response_post):\n",
    "#     if token not in response_deduped_post:\n",
    "#         response_deduped_post.append(token)\n",
    "            \n",
    "# result_dict[\"post\"][\"corpus\"] = response_post\n",
    "# result_dict[\"post\"][\"length\"] = len(response_deduped_post) \n",
    "\n",
    "# with open(\"corpus_prepost.json\", \"w\") as outfile:\n",
    "#     json.dump(result_dict, outfile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e65f185",
   "metadata": {},
   "source": [
    "## Daily full corpus dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3ee7546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = glob.glob('../raw/Full/*.csv')\n",
    "\n",
    "# result_dict = {}\n",
    "# for r in result:\n",
    "#     df = pd.read_csv(r, lineterminator='\\n')\n",
    "#     response = []\n",
    "#     response_deduped = []\n",
    "#     for row in df['lemma']:\n",
    "#         row = ast.literal_eval(row)\n",
    "#         for word in row:\n",
    "#             if word != \" \":\n",
    "#                 response.append(word)\n",
    "#     # Create deduped list to get length of unique words\n",
    "#     for token in response:\n",
    "#         if token not in response_deduped:\n",
    "#             response_deduped.append(token)\n",
    "#     key = re.sub(\".csv$\", \"\", r)\n",
    "#     key = key[-19:]\n",
    "#     result_dict[key] = {}\n",
    "#     result_dict[key][\"corpus\"] = response\n",
    "#     result_dict[key][\"length\"] = len(response_deduped)        \n",
    "\n",
    "# with open(\"corpus_full.json\", \"w\") as outfile:\n",
    "#     json.dump(result_dict, outfile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be491ffb",
   "metadata": {},
   "source": [
    "## Daily Asian corpus dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec448747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = glob.glob('../../raw/consolidated/Asian/*.csv')\n",
    "\n",
    "# result_dict_asian = {}\n",
    "# for r in result:\n",
    "#     df = pd.read_csv(r, lineterminator='\\n')\n",
    "#     response = []\n",
    "#     response_deduped = []\n",
    "#     for row in df['lemma']:\n",
    "#         row = ast.literal_eval(row)\n",
    "#         for word in row:\n",
    "#             if word != \" \":\n",
    "#                 response.append(word)\n",
    "#     # Create deduped list to get length of unique words\n",
    "#     for token in response:\n",
    "#         if token not in response_deduped:\n",
    "#             response_deduped.append(token)\n",
    "#     key = re.sub(\".csv$\", \"\", r)\n",
    "#     key = key[-19:]\n",
    "#     result_dict_asian[key] = {}\n",
    "#     result_dict_asian[key][\"corpus\"] = response\n",
    "#     result_dict_asian[key][\"length\"] = len(response_deduped)        \n",
    "\n",
    "# with open(\"corpus_asian.json\", \"w\") as outfile:\n",
    "#     json.dump(result_dict_asian, outfile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f319dd",
   "metadata": {},
   "source": [
    "## Daily COVID corpus dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340a98f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = glob.glob('../../raw/consolidated/COVID/*.csv')\n",
    "\n",
    "# result_dict_covid = {}\n",
    "# for r in result:\n",
    "#     df = pd.read_csv(r, lineterminator='\\n')\n",
    "#     response = []\n",
    "#     response_deduped = []\n",
    "#     for row in df['lemma']:\n",
    "#         row = ast.literal_eval(row)\n",
    "#         for word in row:\n",
    "#             if word != \" \":\n",
    "#                 response.append(word)\n",
    "#     # Create deduped list to get length of unique words\n",
    "#     for token in response:\n",
    "#         if token not in response_deduped:\n",
    "#             response_deduped.append(token)\n",
    "#     key = re.sub(\".csv$\", \"\", r)\n",
    "#     key = key[-19:]\n",
    "#     result_dict_covid[key] = {}\n",
    "#     result_dict_covid[key][\"corpus\"] = response\n",
    "#     result_dict_covid[key][\"length\"] = len(response_deduped)        \n",
    "\n",
    "# with open(\"corpus_covid.json\", \"w\") as outfile:\n",
    "#     json.dump(result_dict_covid, outfile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d46d12c",
   "metadata": {},
   "source": [
    "## Config dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dad618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config_directory = {}\n",
    "\n",
    "# config = {\"device\": \"cpu\",\n",
    "#           \"window_size\": 15,\n",
    "#           \"num_partitions\": 15,\n",
    "#           \"x_max\": 10,\n",
    "#           \"alpha\": 0.75,\n",
    "#           \"batch_size\": 32,\n",
    "#           \"num_epochs\": 10,\n",
    "#           \"embedding_size\": 50}\n",
    "\n",
    "# for key in result_dict.keys():\n",
    "#     config[\"input_filepath\"] = f\"{key}.txt\"\n",
    "#     config[\"output_filepath\"] = f\"output/{key}.pkl\"\n",
    "#     config[\"vocab_size\"] = result_dict[key][\"length\"]\n",
    "#     config[\"chunk_size\"] = result_dict[key][\"length\"]\n",
    "#     config[\"cooccurrence_dir\"] = f\"cooccurrence_directory/{key}.pkl\"\n",
    "#     config[\"hdf5_file\"] = f\"hdf5_directory/{key}.hdf5\"\n",
    "#     config_directory[key] = config\n",
    "\n",
    "# with open(\"config_prepost.json\", \"w\") as outfile:\n",
    "#     json.dump(config_directory, outfile)\n",
    "\n",
    "# config_directory = {}\n",
    "\n",
    "# for key in result_dict.keys():\n",
    "#     config[\"input_filepath\"] = f\"{key}.txt\"\n",
    "#     config[\"output_filepath\"] = f\"output/{key}.pkl\"\n",
    "#     config[\"vocab_size\"] = result_dict[key][\"length\"]\n",
    "#     config[\"chunk_size\"] = result_dict[key][\"length\"]\n",
    "#     config[\"cooccurrence_dir\"] = f\"cooccurrence_directory/{key}.pkl\"\n",
    "#     config[\"hdf5_file\"] = f\"hdf5_directory/{key}.hdf5\"\n",
    "#     config_directory[key] = config\n",
    "\n",
    "# with open(\"config_full.json\", \"w\") as outfile:\n",
    "#     json.dump(config_directory, outfile)\n",
    "\n",
    "# config_directory_asian = {}\n",
    "\n",
    "# for key in result_dict_asian.keys():\n",
    "#     config_directory_asian[key] = {}\n",
    "#     config[\"input_filepath\"] = f\"{key}.csv\"\n",
    "#     config[\"output_filepath\"] = f\"output/{key}.pkl\"\n",
    "#     config[\"vocab_size\"] = result_dict_asian[key][\"length\"]\n",
    "#     config[\"chunk_size\"] = result_dict_asian[key][\"length\"]\n",
    "#     config[\"cooccurrence_dir\"] = f\"cooccurrence_directory/{key}.pkl\"\n",
    "#     config[\"hdf5_file\"] = f\"hdf5_directory/{key}.hdf5\"\n",
    "#     config_directory_asian[key] = config\n",
    "\n",
    "# with open(\"config_asian.json\", \"w\") as outfile:\n",
    "#     json.dump(config_directory_asian, outfile)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c60fa0",
   "metadata": {},
   "source": [
    "# Stereotypes dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a739640c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stereotypes_df = pd.read_csv(\"Kurdi et al./Kurdi, Mann, Charlesworth, & Banaji (2018) Vectors.csv\")\n",
    "# stereotypes_df = stereotypes_df.groupby('category')['word'].apply(list).to_dict()\n",
    "\n",
    "# stereotypes = {}\n",
    "# keywords = ['Cold', 'Warm', 'Competent', 'Incompetent']\n",
    "# for key in stereotypes_df.keys():\n",
    "#     if key in keywords:\n",
    "#         word_list = stereotypes[key]\n",
    "#         word_list = get_synonyms(word_list)\n",
    "#         for word in word_list:\n",
    "#             if word not in stereotypes[key]:\n",
    "#                 stereotypes[key].append(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdb1ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Stereotypes dict\n",
    "\n",
    "# stereotypes = {\"Cold\": ['cold', 'deceitful', 'dishonest', 'disloyal', 'hateful', 'hostile', 'mean', 'selfish', \n",
    "#                         'unfriendly', 'untrustworthy', 'vicious', 'unsociable', 'unprincipled', 'disagreeable', \n",
    "#                         'egoistic', 'egotistic', 'unkindly', 'unloving', 'inhumane', 'crooked', 'dishonorable', \n",
    "#                         'insincere', 'deceptive', 'thieving', 'corrupt', 'abominable', 'inhospitable', 'ignoble', \n",
    "#                         'stingy', 'contemptible', 'inconsiderate', 'self-serving', 'uncongenial', 'uncordial', \n",
    "#                         'unneighborly', 'devious', 'evil', 'condemnable', 'malicious', 'unsocial', 'antisocial', \n",
    "#                         'ungregarious', 'harsh', 'ill-natured', 'unkind'],\n",
    "               \n",
    "#                \"Warm\": ['warm', 'agreeable', 'dependable', 'reliable', 'friendly', 'good-natured', 'kind', 'nice', \n",
    "#                         'sincere', 'honest', 'supportive', 'trustworthy', 'helpful', 'sociable', 'guileless', \n",
    "#                         'enthusiastic', 'consistent', 'authentic', 'amicable', 'congenial', 'gracious', 'hospitable', \n",
    "#                         'affable', 'neighborly', 'pleasant', 'amiable', 'considerate', 'charitable', 'gentle', \n",
    "#                         'kindhearted', 'forgiving', 'good', 'respectable', 'decent', 'polite', 'courteous', 'genuine', \n",
    "#                         'earnest', 'honorable', 'unpretentious', 'truthful', 'encouraging', 'accommodating', \n",
    "#                         'cooperative', 'extroverted'],\n",
    "               \n",
    "#                \"Competent\": ['able', 'capable', 'competent', 'confident', 'efficient', 'intelligent', 'proficient', \n",
    "#                              'qualified', 'skilled', 'skillful', 'smart', 'motivated', 'persistent', 'resourceful', \n",
    "#                              'effective', 'self-assured', 'certain', 'businesslike', 'cost-efficient', 'expeditious', \n",
    "#                              'streamlined', 'precocious', 'agile', 'brainy', 'bright', 'quick', 'sophisticated', \n",
    "#                              'reasonable', 'rational', 'adept', 'technical', 'well-qualified', 'experienced', \n",
    "#                              'accomplished', 'delicate', 'sure-handed', 'versatile', 'precise', 'astute', 'streetwise', \n",
    "#                              'fastidious', 'driven', 'unforgettable', 'stubborn', 'dogged'],\n",
    "               \n",
    "#                \"Incompetent\": ['dumb', 'foolish', 'helpless', 'ignorant', 'incompetent', 'inefficient', 'inept', \n",
    "#                                'clumsy', 'uncertain', 'unintelligent', 'unqualified', 'unskilled', 'disorganized', \n",
    "#                                'stupid', 'dense', 'inarticulate', 'asinine', 'unwise', 'powerless', 'hopeless', \n",
    "#                                'dependent', 'uneducated', 'uninformed', 'feckless', 'ineffective', 'bungling', 'bad', \n",
    "#                                'inadequate', 'incapable', 'awkward', 'maladroit', 'gawky', 'unpredictable', 'unreliable', \n",
    "#                                'retarded', 'brainless', 'ineligible', 'quack', 'inexperienced', 'weak', 'unprofessional', \n",
    "#                                'amateurish', 'unsystematic', 'chaotic', 'unmethodical'],\n",
    "               \n",
    "#                \"Foreign\": ['foreign', 'alien', 'immigrant', 'extraneous', 'un-american', 'unpatriotic'],\n",
    "               \n",
    "#                \"Diseased\": ['diseased', 'dirty', 'poisonous', 'contagious', 'ill']}\n",
    "\n",
    "# keywords = ['Asians', 'Whites', 'Jews']\n",
    "# for key in stereotypes_df.keys():\n",
    "#     if key in keywords:\n",
    "#         if key not in stereotypes.keys():\n",
    "#             stereotypes[key] = []\n",
    "#         for word in stereotypes_df[key]:\n",
    "#             word = word.lower()\n",
    "#             if word not in stereotypes[key]:\n",
    "#                 stereotypes[key].append(word)\n",
    "        \n",
    "# with open(\"stereotypes.json\", \"w\") as outfile:\n",
    "#     json.dump(stereotypes, outfile)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df387585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Maybe just group positive-negative - use textblob to get polarity\n",
    "# full = []\n",
    "\n",
    "# for stereotype in stereotypes.keys():\n",
    "#     for word in stereotypes[stereotype]:\n",
    "#         if word not in full:\n",
    "#             full.append(word)\n",
    "\n",
    "# positive = get_positive(full)\n",
    "# negative = get_negative(full)\n",
    "\n",
    "# revised['negative'] = negative\n",
    "# revised['positive'] = positive\n",
    "\n",
    "# # stereotype_df = pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in revised.items() ]))\n",
    "# # stereotype_df.to_csv(\"stereotypes.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9151a4f9",
   "metadata": {},
   "source": [
    "# NYT keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c01d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keywords_china = ['Wuhan (China)', 'China', 'Beijing (China)', 'Communist Party of China', 'Xi Jinping', 'Hubei Province (China)', \n",
    "#             'Yunnan Province (China)', 'Anhui (China)', 'Shandong Province (China)', 'Chinese Center for Disease Control and Prevention', \n",
    "#             'Shouguang (China)', 'Sichuan Province (China)', 'Shanghai (China)', 'Chinese Centers for Disease Control and Prevention', \n",
    "#             'Hainan Island (China)', 'Shenzhen (China)', 'Xinhua', 'Tianjin (China)', 'Zhejiang Province (China)', \n",
    "#             \"National People's Congress (China)\", 'Chengdu (China)', 'Hangzhou (China)', 'Yichang (China)', \n",
    "#             'Communist Youth League (China)', 'Zuoling (China)', 'Guangzhou (China)', 'Tibet', 'Xinjiang (China)', \n",
    "#             'Beijing News, The', 'Cyberspace Administration of China', \"Ministry of Public Security of the People's Republic of China\", \n",
    "#             'China Daily', 'China Radio International', 'Uighurs (Chinese Ethnic Group)', 'China Central Television', \n",
    "#             'Zhang, Wei (Epidemiologist)', 'Lunar New Year', 'Hefei (China)', 'Harbin (China)', 'Henan Province (China)', \n",
    "#             'Tiananmen Square (Beijing)', 'Hmong Tribe', 'Guangxi (China)', 'National Bureau of Statistics (China)', \n",
    "#             'Xingcheng (China)', 'Hotan (China)', \"Ministry of State Security of the People's Republic of China\", \n",
    "#             'Xuzhou (China)', 'Urumqi (China)', 'Kashgar (China)', 'Hong Kong Protests (2019)', 'Mao Zedong', \n",
    "#             'Chinese Academy of Sciences', 'Gansu Province (China)', \"People's Bank of China\", 'Changmingzhen (China)', \n",
    "#             'Wuhan Institute of Virology (China)', 'Chinese-Americans', 'Chinatown (Manhattan, NY)', \n",
    "#             'Museum of Chinese in America', 'Far East, South and Southeast Asia and Pacific Areas', 'East Asia', \n",
    "#             'Central Asia']\n",
    "\n",
    "# keywords_asia = ['Wuhan (China)', 'China', 'Beijing (China)', 'Communist Party of China', 'Xi Jinping', 'Hubei Province (China)', \n",
    "#             'Yunnan Province (China)', 'Anhui (China)', 'Shandong Province (China)', 'Chinese Center for Disease Control and Prevention', \n",
    "#             'Shouguang (China)', 'Sichuan Province (China)', 'Shanghai (China)', 'Chinese Centers for Disease Control and Prevention', \n",
    "#             'Hainan Island (China)', 'Shenzhen (China)', 'Xinhua', 'Tianjin (China)', 'Zhejiang Province (China)', \n",
    "#             \"National People's Congress (China)\", 'Chengdu (China)', 'Hangzhou (China)', 'Yichang (China)', \n",
    "#             'Communist Youth League (China)', 'Zuoling (China)', 'Guangzhou (China)', 'Tibet', 'Xinjiang (China)', \n",
    "#             'Beijing News, The', 'Cyberspace Administration of China', \"Ministry of Public Security of the People's Republic of China\", \n",
    "#             'China Daily', 'China Radio International', 'Uighurs (Chinese Ethnic Group)', 'China Central Television', \n",
    "#             'Zhang, Wei (Epidemiologist)', 'Lunar New Year', 'Hefei (China)', 'Harbin (China)', 'Henan Province (China)', \n",
    "#             'Taiwan', 'Taipei (Taiwan)', 'Taoyuan (Taiwan)', 'Tiananmen Square (Beijing)', 'Hmong Tribe', 'Guangxi (China)', \n",
    "#             'National Bureau of Statistics (China)', 'Xingcheng (China)', 'Hotan (China)', \"Ministry of State Security of the People's Republic of China\", \n",
    "#             'Xuzhou (China)', 'Chinese Nationalist Party (Taiwan)', 'Urumqi (China)', 'Kashgar (China)', 'Hong Kong Protests (2019)', \n",
    "#             'Mao Zedong', 'Chinese Academy of Sciences', 'Gansu Province (China)', \"People's Bank of China\", 'Changmingzhen (China)', \n",
    "#             'Wuhan Institute of Virology (China)', 'Chinese-Americans', 'Chinatown (Manhattan, NY)', 'Museum of Chinese in America', \n",
    "#             'Asian-Americans', 'Indian-Americans', 'Vietnamese-Americans', 'Asian-Americans (TV Program)', 'Korean-Americans', \n",
    "#             'Bangladeshi-Americans', 'Far East, South and Southeast Asia and Pacific Areas', 'Southeast Asia', 'East Asia', \n",
    "#             'Central Asia', 'Japan', 'Nara (Japan)', 'Kanazawa (Japan)', 'South Korea', 'Seoul (South Korea)', \n",
    "#             'Jeju Island (South Korea)', 'Daegu (South Korea)', 'Thailand', 'Bangkok (Thailand)', 'Chiang Mai (Thailand)', \n",
    "#             'Lopburi (Thailand)', 'Phuket (Thailand)', 'Singapore', 'Indonesia', 'Bali (Indonesia)', 'Tomohon (Indonesia)', \n",
    "#             'Sulawesi (Indonesia)', 'Java (Indonesia)', 'Surabaya (Indonesia)', 'Maluku Islands (Indonesia)', 'Cambodia', \n",
    "#             'Sihanoukville (Cambodia)', 'Myanmar', 'Yangon (Myanmar)', 'Mandalay (Myanmar)', 'Philippines', 'Manila (Philippines)', \n",
    "#             'Vietnam', 'Ho Chi Minh City (Vietnam)', 'Cam Ranh Bay (Vietnam)', 'North Korea', 'Kaesong (North Korea)', \n",
    "#             'Laos', 'Malaysia', 'Macau', 'Mongolia', 'Nepal', 'Kathmandu (Nepal)', 'Sri Lanka', 'Bangladesh', 'Karachi (Pakistan)', \n",
    "#             'Bhutan','India', 'Bharatiya Janata Party', 'New Delhi (India)', 'Kerala (India)', 'Rajasthan (India)', \n",
    "#             'Uttar Pradesh State (India)', 'Delhi (India)', 'Mumbai (India)', 'Kashmir and Jammu (India)', 'Jaipur (India)', \n",
    "#             'Kashmir Valley (Kashmir and Jammu)',  'Odisha (India)', 'Karnataka (India)', 'Maharashtra (India)', \n",
    "#             'Bay of Bengal', 'Gujarat State (India)', 'Kolkata (India)', 'Bihar (India)', 'Srinagar (Jammu and Kashmir)',\n",
    "#             'AHMEDABAD (INDIA)', 'Himalayas', 'Ladakh (India)', 'Noida (India)', 'Darjeeling (India)', 'Serum Institute of India', \n",
    "#             'Punjab (India)', 'Andhra Pradesh (India)', 'Tamil Nadu (India)', 'Tripura (India)', 'Agartala (India)', \n",
    "#             'West Bengal (India)', 'Dharamsala (India)', 'Nashik (India)', 'Bhopal (India)', 'Goa (India)', 'Pune (India)', \n",
    "#             'Public Health Foundation of India']\n",
    "        \n",
    "# keywords_political = ['Trump, Donald J', 'Conservative Political Action Conference', 'Republican Party', \n",
    "#                       'Republican National Committee', 'Republican National Convention', 'Democratic Party', \n",
    "#                       'Democratic National Committee', 'Democratic National Convention']\n",
    "\n",
    "# nyt_df = {}\n",
    "# for result in results.keys():\n",
    "#     if result not in nyt_df.keys():\n",
    "#         nyt_df[result] = {}\n",
    "#         nyt_df[result]['total'] = len(results[result])\n",
    "#         chinese = 0\n",
    "#         asian = 0\n",
    "#         political = 0\n",
    "#         if len(results[result]) > 0:\n",
    "#             for article in results[result]:\n",
    "#                 chinese_article = 0\n",
    "#                 asian_article = 0\n",
    "#                 political_article = 0\n",
    "#                 for keyword in article['keywords']:\n",
    "#                     if keyword['value'] in keywords_china:\n",
    "#                         chinese_article += 1\n",
    "#                     if keyword['value'] in keywords_asia:\n",
    "#                         asian_article += 1\n",
    "#                     if keyword['value'] in keywords_political:\n",
    "#                         political_article += 1\n",
    "#                 if chinese_article > 0:\n",
    "#                     chinese += 1\n",
    "#                 if asian_article > 0:\n",
    "#                     asian += 1\n",
    "#                 if political_article > 0:\n",
    "#                     political += 1\n",
    "#         nyt_df[result]['chinese'] = chinese\n",
    "#         nyt_df[result]['asian'] = asian\n",
    "#         nyt_df[result]['political'] = political\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b4db82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_nyt = pd.DataFrame.from_dict({(i): nyt_df[i] \n",
    "#                                  for i in nyt_df.keys()},\n",
    "#                                 orient='index')\n",
    "# df_nyt.reset_index(inplace=True)\n",
    "# df_nyt = df_nyt.rename(columns = {'index':'date'})\n",
    "# df_nyt = df_nyt.sort_values(by=['date'], ignore_index=True)\n",
    "\n",
    "\n",
    "# df_nyt.head()\n",
    "\n",
    "# df_nyt.to_csv(\"df_nyt.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bee064",
   "metadata": {},
   "source": [
    "# For LIWC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4298af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # List all files in the BERT folder\n",
    "# results = glob('../raw/Full/*.csv')\n",
    "# results.sort()\n",
    "\n",
    "# f = open(\"corpus_full.json\")\n",
    "# corpus_full = json.load(f)\n",
    "# f.close()\n",
    "\n",
    "# full_raw = {}\n",
    "\n",
    "# for r in results:\n",
    "#     day = r[-14:-4]\n",
    "#     print(day)\n",
    "#     if day not in full_raw.keys():\n",
    "#         full_raw[day] = {}\n",
    "#     full_raw[day]['sentence'] = []\n",
    "#     full_raw[day]['user'] = []\n",
    "    \n",
    "#     df = pd.read_csv(r, lineterminator='\\n')\n",
    "#     df = df[df.lemma_length > 0]\n",
    "    \n",
    "#     corpus = df.text.tolist()\n",
    "#     users = df.author_id.tolist()\n",
    "    \n",
    "#     for user, sentence in zip(users, corpus):\n",
    "#         sentence = preprocess(sentence)\n",
    "#         full_raw[day]['sentence'].append(sentence)\n",
    "#         full_raw[day]['user'].append(user)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca2c919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_raw_df = pd.DataFrame.from_dict({(i): full_raw[i]\n",
    "#                                       for i in full_raw.keys()}, \n",
    "#                                      orient=\"index\")\n",
    "\n",
    "# full_raw_df = full_raw_df.explode(['sentence', 'user'])\n",
    "# full_raw_df = full_raw_df.reset_index()\n",
    "# full_raw_df.columns = [\"day\", \"sentence\", \"user\"]\n",
    "\n",
    "# full_raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8609c32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_raw_df.to_csv(\"LIWC_df.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
