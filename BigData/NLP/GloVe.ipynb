{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import ast\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from dataclasses import dataclass, field\n",
    "import contextlib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus.reader import CorpusReader\n",
    "from nltk.internals import deprecated\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.util import binary_search_file as _binary_search_file\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from textblob import TextBlob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.optim\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import h5py\n",
    "from gensim.models.keyedvectors import KeyedVectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define classes and functions for GloVe\n",
    "@dataclass\n",
    "class Vocabulary:\n",
    "    token2index: dict = field(default_factory=dict)\n",
    "    index2token: dict = field(default_factory=dict)\n",
    "    token_counts: list = field(default_factory=list)\n",
    "    _unk_token: int = field(init=False, default=-1)\n",
    "    ### Define add\n",
    "    def add(self, token):\n",
    "        if token not in self.token2index:\n",
    "            index = len(self)\n",
    "            self.token2index[token] = index\n",
    "            self.index2token[index] = token\n",
    "            self.token_counts.append(0)\n",
    "        self.token_counts[self.token2index[token]] += 1\n",
    "    ### Define top k tokens\n",
    "    def get_topk_subset(self, k):\n",
    "        tokens = sorted(list(self.token2index.keys()),\n",
    "                        key=lambda token: self.token_counts[self[token]],\n",
    "                        reverse=True)\n",
    "        return type(self)(token2index={token: index for index, token in enumerate(tokens[:k])},\n",
    "                          index2token={index: token for index, token in enumerate(tokens[:k])},\n",
    "                          token_counts=[self.token_counts[self.token2index[token]] for token in tokens[:k]])\n",
    "    ### Define shuffle\n",
    "    def shuffle(self):\n",
    "        new_index = [_ for _ in range(len(self))]\n",
    "        random.shuffle(new_index)\n",
    "        new_token_counts = [None] * len(self)\n",
    "        for token, index in zip(list(self.token2index.keys()), new_index):\n",
    "            new_token_counts[index] = self.token_counts[self[token]]\n",
    "            self.token2index[token] = index\n",
    "            self.index2token[index] = token\n",
    "        self.token_counts = new_token_counts\n",
    "    ### Define get index\n",
    "    def get_index(self, token):\n",
    "        return self[token]\n",
    "    ### Define get token\n",
    "    def get_token(self, index):\n",
    "        if not index in self.index2token:\n",
    "            raise Exception(\"Invalid index.\")\n",
    "        return self.index2token[index]\n",
    "    ### Define unknown token\n",
    "    @property\n",
    "    def unk_token(self):\n",
    "        return self._unk_token\n",
    "    ### Define getitem\n",
    "    def __getitem__(self, token):\n",
    "        if token not in self.token2index:\n",
    "            return self._unk_token\n",
    "        return self.token2index[token]\n",
    "    ### Define len\n",
    "    def __len__(self):\n",
    "        return len(self.token2index)\n",
    "\n",
    "\n",
    "### Define Vectorizer\n",
    "@dataclass\n",
    "class Vectorizer:\n",
    "    vocab: Vocabulary\n",
    "    ### Define from_corpus\n",
    "    @classmethod\n",
    "    def from_corpus(cls, corpus, vocab_size):\n",
    "        vocab = Vocabulary()\n",
    "        for token in corpus:\n",
    "            vocab.add(token)\n",
    "        vocab_subset = vocab.get_topk_subset(vocab_size)\n",
    "        vocab_subset.shuffle()\n",
    "        return cls(vocab_subset)\n",
    "    ### Define vectorize\n",
    "    def vectorize(self, corpus):\n",
    "        return [self.vocab[token] for token in corpus]\n",
    "\n",
    "### Define Cooccurrence Entries\n",
    "@dataclass\n",
    "class CooccurrenceEntries:\n",
    "    vectorized_corpus: list\n",
    "    vectorizer: Vectorizer\n",
    "    ### Define setup\n",
    "    @classmethod\n",
    "    def setup(cls, corpus, vectorizer):\n",
    "        return cls(vectorized_corpus=vectorizer.vectorize(corpus),\n",
    "                   vectorizer=vectorizer)\n",
    "    ### Define validate index\n",
    "    def validate_index(self, index, lower, upper):\n",
    "        is_unk = index == self.vectorizer.vocab.unk_token\n",
    "        if lower < 0:\n",
    "            return not is_unk\n",
    "        return not is_unk and index >= lower and index <= upper\n",
    "    ### Define build\n",
    "    def build(self,\n",
    "              window_size,\n",
    "              num_partitions,\n",
    "              chunk_size,\n",
    "              output_directory=\".\",\n",
    "              hdf5_directory = \".\"):\n",
    "        partition_step = len(self.vectorizer.vocab) // num_partitions\n",
    "        split_points = [0]\n",
    "        while split_points[-1] + partition_step <= len(self.vectorizer.vocab):\n",
    "            split_points.append(split_points[-1] + partition_step)\n",
    "        split_points[-1] = len(self.vectorizer.vocab)\n",
    "        for partition_id in tqdm(range(len(split_points) - 1)):\n",
    "            index_lower = split_points[partition_id]\n",
    "            index_upper = split_points[partition_id + 1] - 1\n",
    "            cooccurr_counts = Counter()\n",
    "            for i in tqdm(range(len(self.vectorized_corpus))):\n",
    "                if not self.validate_index(self.vectorized_corpus[i],\n",
    "                                           index_lower,\n",
    "                                           index_upper):\n",
    "                    continue\n",
    "                \n",
    "                context_lower = max(i - window_size, 0)\n",
    "                context_upper = min(i + window_size + 1, len(self.vectorized_corpus))\n",
    "                for j in range(context_lower, context_upper):\n",
    "                    if i == j or not self.validate_index(self.vectorized_corpus[j],-1,-1):\n",
    "                        continue\n",
    "                    cooccurr_counts[(self.vectorized_corpus[i], self.vectorized_corpus[j])] += 1 / abs(i - j)\n",
    "            cooccurr_dataset = np.zeros((len(cooccurr_counts), 3))\n",
    "            for index, ((i, j), cooccurr_count) in enumerate(cooccurr_counts.items()):\n",
    "                cooccurr_dataset[index] = (i, j, cooccurr_count)\n",
    "            if partition_id == 0:\n",
    "                file = h5py.File(hdf5_directory,\"w\")\n",
    "                dataset = file.create_dataset(\"cooccurrence\",\n",
    "                                              (len(cooccurr_counts), 3),\n",
    "                                              maxshape=(None, 3),\n",
    "                                              chunks=(chunk_size, 3))\n",
    "                prev_len = 0\n",
    "            else:\n",
    "                prev_len = dataset.len()\n",
    "                dataset.resize(dataset.len() + len(cooccurr_counts), axis=0)\n",
    "            dataset[prev_len: dataset.len()] = cooccurr_dataset\n",
    "        file.close()\n",
    "        with open(output_directory, \"wb\") as file:\n",
    "            pickle.dump(self.vectorizer.vocab, file)\n",
    "\n",
    "### Define Coocurrence Dataset\n",
    "@dataclass\n",
    "class CooccurrenceDataset(torch.utils.data.Dataset):\n",
    "    token_ids: torch.Tensor\n",
    "    cooccurr_counts: torch.Tensor\n",
    "    ### Define getitem\n",
    "    def __getitem__(self, index):\n",
    "        return [self.token_ids[index], self.cooccurr_counts[index]]\n",
    "    ### Define len\n",
    "    def __len__(self):\n",
    "        return self.token_ids.size()[0]\n",
    "\n",
    "### Define Calculate Coocurrence     \n",
    "def calculate_cooccurrence(corpus, config):\n",
    "    ### Use vectorizer defined above\n",
    "    vectorizer = Vectorizer.from_corpus(corpus=corpus,\n",
    "                                        vocab_size=config[\"vocab_size\"])\n",
    "    ### Use cooccurrence entries defined above\n",
    "    cooccurrence = CooccurrenceEntries.setup(corpus=corpus,\n",
    "                                             vectorizer=vectorizer)\n",
    "    cooccurrence.build(window_size=config[\"window_size\"],\n",
    "                       num_partitions=config[\"num_partitions\"],\n",
    "                       chunk_size=config[\"chunk_size\"],\n",
    "                       output_directory=config[\"cooccurrence_dir\"], \n",
    "                       hdf5_directory = config[\"hdf5_file\"]) \n",
    "\n",
    "\n",
    "### Define data loader\n",
    "@dataclass\n",
    "class HDF5DataLoader:\n",
    "    filepath: str\n",
    "    dataset_name: str\n",
    "    batch_size: int\n",
    "    device: str\n",
    "    dataset: h5py.Dataset = field(init=False)\n",
    "    ### Define iter_batches\n",
    "    def iter_batches(self):\n",
    "        chunks = list(self.dataset.iter_chunks())\n",
    "        random.shuffle(chunks)\n",
    "        for chunk in chunks:\n",
    "            chunked_dataset = self.dataset[chunk]\n",
    "            dataloader = torch.utils.data.DataLoader(dataset=CooccurrenceDataset(token_ids=torch.from_numpy(chunked_dataset[:,:2]).long(),\n",
    "                                                                                 cooccurr_counts=torch.from_numpy(chunked_dataset[:,2]).float()),\n",
    "                                                     batch_size=self.batch_size,\n",
    "                                                     shuffle=True,\n",
    "                                                     pin_memory=True)\n",
    "            for batch in dataloader:\n",
    "                batch = [_.to(self.device) for _ in batch]\n",
    "                yield batch  \n",
    "    ### Define open\n",
    "    @contextlib.contextmanager\n",
    "    def open(self):\n",
    "        with h5py.File(self.filepath, \"r\") as file:\n",
    "            self.dataset = file[self.dataset_name]\n",
    "            yield\n",
    "\n",
    "\n",
    "### Define GloVe\n",
    "class GloVe(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, x_max, alpha):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Embedding(num_embeddings=vocab_size,\n",
    "                                   embedding_dim=embedding_size,\n",
    "                                   sparse=True)\n",
    "        self.weight_tilde = nn.Embedding(num_embeddings=vocab_size,\n",
    "                                         embedding_dim=embedding_size,\n",
    "                                         sparse=True)\n",
    "        self.bias = nn.Parameter(torch.randn(vocab_size,\n",
    "                                             dtype=torch.float,))\n",
    "        self.bias_tilde = nn.Parameter(torch.randn(vocab_size,\n",
    "                                                   dtype=torch.float,))\n",
    "        self.weighting_func = lambda x: (x / x_max).float_power(alpha).clamp(0, 1)\n",
    "    def forward(self, i, j, x):\n",
    "        loss = torch.mul(self.weight(i), self.weight_tilde(j)).sum(dim=1)\n",
    "        loss = (loss + self.bias[i] + self.bias_tilde[j] - x.log()).square()\n",
    "        loss = torch.mul(self.weighting_func(x), loss).mean()\n",
    "        return loss\n",
    "\n",
    "### Define train glove\n",
    "def train_glove(config):\n",
    "    dataloader = HDF5DataLoader(filepath=config[\"hdf5_file\"],\n",
    "                                dataset_name=\"cooccurrence\",\n",
    "                                batch_size=config[\"batch_size\"],\n",
    "                                device=config[\"device\"])\n",
    "    model = GloVe(vocab_size=config[\"vocab_size\"],\n",
    "                  embedding_size=config[\"embedding_size\"],\n",
    "                  x_max=config[\"x_max\"],\n",
    "                  alpha=config[\"alpha\"])\n",
    "    model.to(config[\"device\"])\n",
    "    optimizer = torch.optim.Adagrad(model.parameters())\n",
    "    with dataloader.open():\n",
    "        model.train()\n",
    "        losses = []\n",
    "        for epoch in tqdm(range(config[\"num_epochs\"])):\n",
    "            epoch_loss = 0\n",
    "            for batch in tqdm(dataloader.iter_batches()):\n",
    "                loss = model(batch[0][:, 0],\n",
    "                             batch[0][:, 1],\n",
    "                             batch[1])\n",
    "                epoch_loss += loss.detach().item()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            losses.append(epoch_loss)\n",
    "            print(f\"Epoch {epoch}: loss = {epoch_loss}\")\n",
    "            torch.save(model.state_dict(), config[\"output_filepath\"])\n",
    "    plt.plot(losses)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open corpus_prepost json as dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"corpus_prepost.json\")\n",
    "result_dict = json.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"config_prepost.json\")\n",
    "config = json.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key in result_dict.keys():\n",
    "#     corpus = result_dict[key][\"corpus\"]\n",
    "#     config[\"input_filepath\"] = f\"{key}.csv\"\n",
    "#     config[\"output_filepath\"] = f\"output/{key}.pkl\"\n",
    "#     config[\"vocab_size\"] = result_dict[key][\"length\"]\n",
    "#     config[\"chunk_size\"] = result_dict[key][\"length\"]\n",
    "#     config[\"cooccurrence_dir\"] = f\"cooccurrence_directory/{key}.pkl\"\n",
    "#     config[\"hdf5_file\"] = f\"hdf5_directory/{key}.hdf5\"\n",
    "#     # only train for files that haven't been trained yet\n",
    "#     if not os.path.isfile(config[\"output_filepath\"]):\n",
    "#         calculate_cooccurrence(corpus, config)\n",
    "#         train_glove(config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open corpus_full json as dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"corpus_full.json\")\n",
    "result_dict = json.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"config_full.json\")\n",
    "config = json.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key in result_dict.keys():\n",
    "#     corpus = result_dict[key][\"corpus\"]\n",
    "#     config[\"input_filepath\"] = f\"{key}.csv\"\n",
    "#     config[\"output_filepath\"] = f\"output/{key}.pkl\"\n",
    "#     config[\"vocab_size\"] = result_dict[key][\"length\"]\n",
    "#     config[\"chunk_size\"] = result_dict[key][\"length\"]\n",
    "#     config[\"cooccurrence_dir\"] = f\"cooccurrence_directory/{key}.pkl\"\n",
    "#     config[\"hdf5_file\"] = f\"hdf5_directory/{key}.hdf5\"\n",
    "#     # only train for files that haven't been trained yet\n",
    "#     if not os.path.isfile(config[\"output_filepath\"]):\n",
    "#         calculate_cooccurrence(corpus, config)\n",
    "#         train_glove(config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open corpus_asian json as dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"corpus_asian.json\")\n",
    "result_dict_asian = json.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"config_asian.json\")\n",
    "config = json.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key in result_dict_asian.keys():\n",
    "#     corpus = result_dict_asian[key][\"corpus\"]\n",
    "#     config[\"input_filepath\"] = f\"{key}.csv\"\n",
    "#     config[\"output_filepath\"] = f\"output/{key}_asian.pkl\"\n",
    "#     config[\"vocab_size\"] = result_dict_asian[key][\"length\"]\n",
    "#     config[\"chunk_size\"] = result_dict_asian[key][\"length\"]\n",
    "#     config[\"cooccurrence_dir\"] = f\"cooccurrence_directory/{key}_asian.pkl\"\n",
    "#     config[\"hdf5_file\"] = f\"hdf5_directory/{key}_asian.hdf5\"\n",
    "#     # only train for files that haven't been trained yet\n",
    "#     if not os.path.isfile(config[\"output_filepath\"]):\n",
    "#         calculate_cooccurrence(corpus, config)\n",
    "#         train_glove(config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open corpus_covid json as dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = open(\"corpus_covid.json\")\n",
    "# result_dict_covid = json.load(f)\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = {\"device\": \"cpu\",\n",
    "#           \"window_size\": 15,\n",
    "#           \"num_partitions\": 15,\n",
    "#           \"x_max\": 10,\n",
    "#           \"alpha\": 0.75,\n",
    "#           \"batch_size\": 32,\n",
    "#           \"num_epochs\": 10,\n",
    "#           \"embedding_size\": 50}\n",
    "\n",
    "# for key in result_dict_covid.keys():\n",
    "#     corpus = result_dict_covid[key][\"corpus\"]\n",
    "#     config[\"input_filepath\"] = f\"{key}.csv\"\n",
    "#     config[\"output_filepath\"] = f\"output/{key}_covid.pkl\"\n",
    "#     config[\"vocab_size\"] = result_dict_covid[key][\"length\"]\n",
    "#     config[\"chunk_size\"] = result_dict_covid[key][\"length\"]\n",
    "#     config[\"cooccurrence_dir\"] = f\"cooccurrence_directory/{key}_covid.pkl\"\n",
    "#     config[\"hdf5_file\"] = f\"hdf5_directory/{key}_covid.hdf5\"\n",
    "#     # only train for files that haven't been trained yet\n",
    "#     if not os.path.isfile(config[\"output_filepath\"]):\n",
    "#         calculate_cooccurrence(corpus, config)\n",
    "#         train_glove(config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect the vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the stereotypes json\n",
    "\n",
    "# f = open(\"stereotypes.json\")\n",
    "# stereotypes = json.load(f)\n",
    "# f.close()\n",
    "\n",
    "# Open the config jsons\n",
    "\n",
    "# f = open(\"config_prepost.json\")\n",
    "# config_prepost = json.load(f)\n",
    "# f.close()\n",
    "\n",
    "# f = open(\"config_full.json\")\n",
    "# config_full = json.load(f)\n",
    "# f.close()\n",
    "\n",
    "# f = open(\"config_asian.json\")\n",
    "# config_asian = json.load(f)\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre Post corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings_prepost = {}\n",
    "\n",
    "# path_to_folder = \"output/\"\n",
    "# matching_files = glob.glob(path_to_folder+\"*.pkl\")\n",
    "\n",
    "# for matching_file in matching_files:\n",
    "#     new_name = matching_file[7:-4]\n",
    "#     embeddings_prepost[new_name] = {}\n",
    "#     embeddings_prepost[new_name]['database'] = []\n",
    "#     embeddings_prepost[new_name]['category'] = []\n",
    "#     embeddings_prepost[new_name]['word'] = []\n",
    "#     embeddings_prepost[new_name]['vectors'] = []\n",
    "    \n",
    "#     # Load cooccurrence directory\n",
    "#     cooc_outfile = config_prepost[new_name][\"cooccurrence_dir\"]\n",
    "#     with open(cooc_outfile, \"rb\") as f:\n",
    "#         vocab = pickle.load(f)\n",
    "    \n",
    "#     # Load model\n",
    "#     model = GloVe(vocab_size=config_prepost[new_name][\"vocab_size\"],\n",
    "#                   embedding_size=config_prepost[new_name][\"embedding_size\"],\n",
    "#                   x_max=config_prepost[new_name][\"x_max\"],\n",
    "#                   alpha=config_prepost[new_name][\"alpha\"])\n",
    "#     outfile = config_prepost[new_name][\"output_filepath\"]\n",
    "#     model.load_state_dict(torch.load(outfile)) \n",
    "\n",
    "#     # Get Keyed Vectors\n",
    "#     keyed_vectors = KeyedVectors(vector_size=config_prepost[new_name][\"embedding_size\"])\n",
    "#     keyed_vectors.add_vectors(keys=[vocab.get_token(index) for index in range(config_prepost[new_name][\"vocab_size\"])],\n",
    "#                               weights=(model.weight.weight.detach() + model.weight_tilde.weight.detach()).numpy())\n",
    "\n",
    "#     # Go through all the categories\n",
    "#     for key in stereotypes.keys():\n",
    "#         # Go through all the words\n",
    "#         for word in stereotypes[key]:\n",
    "#             word = word.lower()\n",
    "#             # Get vectors\n",
    "#             if keyed_vectors.__contains__(word):\n",
    "#                 weights = keyed_vectors.get_vector(word, norm=False)\n",
    "#                 weights = weights.tolist()\n",
    "#                 # Now append everything\n",
    "#                 embeddings_prepost[new_name]['database'].append(\"glove\")\n",
    "#                 embeddings_prepost[new_name]['category'].append(key)\n",
    "#                 embeddings_prepost[new_name]['word'].append(word)\n",
    "#                 embeddings_prepost[new_name]['vectors'].append(weights)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings_full = {}\n",
    "\n",
    "# path_to_folder = \"output/Full/\"\n",
    "# matching_files = glob.glob(path_to_folder+\"*.pkl\")\n",
    "# for matching_file in matching_files:\n",
    "#     new_name = matching_file[-14:-4]\n",
    "#     config_name = f\"{matching_file[-23:-4]}\"\n",
    "#     embeddings_full[new_name] = {}\n",
    "#     embeddings_full[new_name]['database'] = []\n",
    "#     embeddings_full[new_name]['category'] = []\n",
    "#     embeddings_full[new_name]['word'] = []\n",
    "#     embeddings_full[new_name]['vectors'] = []\n",
    "    \n",
    "#     # Load cooccurrence directory\n",
    "#     cooc_outfile = config_full[config_name][\"cooccurrence_dir\"]\n",
    "#     vocab_file = f\"{cooc_outfile[0:23]}Full{cooc_outfile[22:]}\"\n",
    "#     with open(vocab_file, \"rb\") as f:\n",
    "#         vocab = pickle.load(f)\n",
    "#     # Load model\n",
    "#     model = GloVe(vocab_size=config_full[config_name][\"vocab_size\"],\n",
    "#                   embedding_size=config_full[config_name][\"embedding_size\"],\n",
    "#                   x_max=config_full[config_name][\"x_max\"],\n",
    "#                   alpha=config_full[config_name][\"alpha\"])\n",
    "#     outfile = config_full[config_name][\"output_filepath\"]\n",
    "#     model_file = f\"{outfile[0:7]}Full{outfile[6:]}\"\n",
    "#     model.load_state_dict(torch.load(model_file)) \n",
    "\n",
    "#     # Get Keyed Vectors\n",
    "#     keyed_vectors = KeyedVectors(vector_size=config_full[config_name][\"embedding_size\"])\n",
    "#     keyed_vectors.add_vectors(keys=[vocab.get_token(index) for index in range(config_full[config_name][\"vocab_size\"])],\n",
    "#                               weights=(model.weight.weight.detach() + model.weight_tilde.weight.detach()).numpy())\n",
    "#     # Go through all the categories\n",
    "#     for key in stereotypes.keys():\n",
    "#         # Go through all the words\n",
    "#         for word in stereotypes[key]:\n",
    "#             word = word.lower()\n",
    "#             # Get vectors\n",
    "#             if keyed_vectors.__contains__(word):\n",
    "#                 weights = keyed_vectors.get_vector(word, norm=False)\n",
    "#                 weights = weights.tolist()\n",
    "#                 # Now append everything\n",
    "#                 embeddings_full[new_name]['database'].append(\"glove\")\n",
    "#                 embeddings_full[new_name]['category'].append(key)\n",
    "#                 embeddings_full[new_name]['word'].append(word)\n",
    "#                 embeddings_full[new_name]['vectors'].append(weights)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asian corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings_asian = {}\n",
    "\n",
    "# path_to_folder = \"output/Asian/\"\n",
    "# matching_files = glob.glob(path_to_folder+\"*.pkl\")\n",
    "# for matching_file in matching_files:\n",
    "#     new_name = matching_file[-20:-10]\n",
    "#     config_name = f\"{matching_file[-29:-10]}\"\n",
    "#     embeddings_asian[new_name] = {}\n",
    "#     embeddings_asian[new_name]['database'] = []\n",
    "#     embeddings_asian[new_name]['category'] = []\n",
    "#     embeddings_asian[new_name]['word'] = []\n",
    "#     embeddings_asian[new_name]['vectors'] = []\n",
    "    \n",
    "#     # Load cooccurrence directory\n",
    "#     cooc_outfile = config_asian[config_name][\"cooccurrence_dir\"]\n",
    "#     vocab_file = f\"{cooc_outfile[0:23]}Asian{cooc_outfile[22:-4]}_asian.pkl\"\n",
    "#     with open(vocab_file, \"rb\") as f:\n",
    "#         vocab = pickle.load(f)\n",
    "#     # Load model\n",
    "#     model = GloVe(vocab_size=config_asian[config_name][\"vocab_size\"],\n",
    "#                   embedding_size=config_asian[config_name][\"embedding_size\"],\n",
    "#                   x_max=config_asian[config_name][\"x_max\"],\n",
    "#                   alpha=config_asian[config_name][\"alpha\"])\n",
    "#     outfile = config_asian[config_name][\"output_filepath\"]\n",
    "#     model_file = f\"{outfile[0:7]}Asian{outfile[6:-4]}_asian.pkl\"\n",
    "#     model.load_state_dict(torch.load(model_file)) \n",
    "\n",
    "#     # Get Keyed Vectors\n",
    "#     keyed_vectors = KeyedVectors(vector_size=config_asian[config_name][\"embedding_size\"])\n",
    "#     keyed_vectors.add_vectors(keys=[vocab.get_token(index) for index in range(config_asian[config_name][\"vocab_size\"])],\n",
    "#                               weights=(model.weight.weight.detach() + model.weight_tilde.weight.detach()).numpy())\n",
    "#     # Go through all the categories\n",
    "#     for key in stereotypes.keys():\n",
    "#         # Go through all the words\n",
    "#         for word in stereotypes[key]:\n",
    "#             word = word.lower()\n",
    "#             # Get vectors\n",
    "#             if keyed_vectors.__contains__(word):\n",
    "#                 weights = keyed_vectors.get_vector(word)\n",
    "#                 weights = weights.tolist()\n",
    "#                 # Now append everything\n",
    "#                 embeddings_asian[new_name]['database'].append(\"glove\")\n",
    "#                 embeddings_asian[new_name]['category'].append(key)\n",
    "#                 embeddings_asian[new_name]['word'].append(word)\n",
    "#                 embeddings_asian[new_name]['vectors'].append(weights)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_prepost = pd.DataFrame(columns = ['day', 'database', 'category', 'word', 'vectors'])\n",
    "\n",
    "# for day in embeddings_prepost.keys():\n",
    "#     df_prepost_day = pd.DataFrame.from_dict(embeddings_prepost[day], orient=\"columns\")\n",
    "#     df_prepost_day['day'] = day\n",
    "#     df_prepost = df_prepost.append(df_prepost_day)\n",
    "    \n",
    "# df_prepost = df_prepost.sort_values(by=\"day\")\n",
    "\n",
    "# df_prepost.to_csv('embeddings_prepost.csv', index = False)\n",
    "\n",
    "## Check duplicates as sanity check\n",
    "# df_prepost.loc[df_prepost['vectors'].astype(str).drop_duplicates().index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_full = pd.DataFrame(columns = ['day', 'database', 'category', 'word', 'vectors'])\n",
    "\n",
    "# for day in embeddings_full.keys():\n",
    "#     df_full_day = pd.DataFrame.from_dict(embeddings_full[day], orient=\"columns\")\n",
    "#     df_full_day['day'] = day\n",
    "#     df_full = df_full.append(df_full_day)\n",
    "    \n",
    "# df_full = df_full.sort_values(by=\"day\")\n",
    "\n",
    "# df_full.to_csv('embeddings_full.csv', index = False)\n",
    "\n",
    "## Check duplicates as sanity check\n",
    "# df_full.loc[df_full['vectors'].astype(str).drop_duplicates().index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_asian = pd.DataFrame(columns = ['day', 'database', 'category', 'word', 'vectors'])\n",
    "\n",
    "# for day in embeddings_asian.keys():\n",
    "#     df_asian_day = pd.DataFrame.from_dict(embeddings_asian[day], orient=\"columns\")\n",
    "#     df_asian_day['day'] = day\n",
    "#     df_asian = df_asian.append(df_asian_day)\n",
    "\n",
    "# df_asian = df_asian.sort_values(by=\"day\")\n",
    "\n",
    "# df_asian.to_csv('embeddings_asian.csv', index = False)\n",
    "\n",
    "## Check duplicates as sanity check\n",
    "# df_asian.loc[df_asian['vectors'].astype(str).drop_duplicates().index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
